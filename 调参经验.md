# 在卷积神经网络训练过程中loss出现NaN的原因以及可以采取的方法

## 1.梯度爆炸

------

**原因**：在学习过程中，梯度变得非常大，使得学习的过程偏离了正常的轨迹。

**症状**：观察输出日志(runtime log)中每次迭代的loss值，你会发现loss随着迭代有明显的增长，最后因为loss值太大以致于不能用浮点数去表示，所以变成了*NaN*。

**可采取的方法**：1.降低学习率，比如*solver.prototxt*中*base_lr*，降低一个数量级（至少）。如果在你的模型中有多个loss层，就不能降低基础的学习率*base_lr*，而是需要检查日志，找到产生梯度爆炸的层，然后降低*train_val.prototxt*中该层的*loss_weight*。

## 2.错误的学习率策略及参数

------

**原因**：在学习过程中，caffe不能得出一个正确的学习率，相反会得到*inf*或者*nan*的值。这些错误的学习率乘上所有的梯度使得所有参数变成无效的值。

**症状**：观察输出日志(runtime log)，你应该可以看到学习率变成*NaN*，例如：

```
... sgd_solver.cpp:106] Iteration 0, lr = -nan1
```

**可采取的方法**：修改*solver.prototxt*文件中所有能影响学习率的参数。比如，如果你设置的学习率策略是 *lr_policy: “poly”* ，而你又忘了设置最大迭代次数*max_iter*，那么最后你会得到*lr=NaN*…

关于caffe学习率及其策略的内容，可以在github的*/caffe-master/src/caffe/proto/caffe.proto* 文件中看到 ([传送门](https://github.com/BVLC/caffe/blob/master/src/caffe/proto/caffe.proto#L157-L172))。 
下面是源文件的注释部分的描述：

```
// The learning rate decay policy. The currently implemented learning rate
// policies are as follows:
//    - fixed: always return base_lr.
//    - step: return base_lr * gamma ^ (floor(iter / step))
//    - exp: return base_lr * gamma ^ iter
//    - inv: return base_lr * (1 + gamma * iter) ^ (- power)
//    - multistep: similar to step but it allows non uniform steps defined by
//      stepvalue
//    - poly: the effective learning rate follows a polynomial decay, to be
//      zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)
//    - sigmoid: the effective learning rate follows a sigmod decay
//      return base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))
//
// where base_lr, max_iter, gamma, step, stepvalue and power are defined
// in the solver parameter protocol buffer, and iter is the current iteration.123456789101112131415
```

## 3.错误的损失函数

------

**原因**：有时，在损失层计算损失值时会出现*NaN*的情况。比如，向*InfogainLoss*层没有归一化输入值，使用自定义的损失层等。

**症状**：观察输出日志(runtime log)的时候，你可能不会发现任何异常：loss逐渐下降，然后突然出现*NaN*。

**可采取的方法**：尝试重现该错误，打印损失层的值并调试。

举个栗子：有一回，我根据批量数据中标签出现的频率去归一化惩罚值并以此计算loss。如果有个label并没有在批量数据中出现，频率为0，结果loss出现了*NaN*的情况。在这种情况下，需要用足够大的batch来避免这个错误。

## 4.错误的输入

------

**原因**：你的输入中存在*NaN*！

**症状**：一旦学习过程中碰到这种错误的输入，输出就会变成*NaN*。观察输出日志(runtime log)的时候，你可能也不会发现任何异常：loss逐渐下降，然后突然出现*NaN*。

**可采取的方法**：重建你的输入数据集(lmdb/leveldn/hdf5…)，确保你的训练集/验证集中没有脏数据（错误的图片文件）。调试时，使用一个简单的网络去读取输入，如果有一个输入有错误，这个网络的loss也会出现*NaN*。

## 5.Pooling层的步长大于核的尺寸

------

由于一些原因，步长*stride*>核尺寸*kernel_size*的pooling层会出现*NaN*。比如：

```
layer {
  name: "faulty_pooling"
  type: "Pooling"
  bottom: "x"
  top: "y"
  pooling_param {
    pool: AVE
    stride: 5
    kernel: 3
  }
}
```

# LSTM单元精讲

<http://www.mtcnn.com/?p=529>

本文只是介绍tensorflow中的tf.nn.rnn_cell.LSTMCell中num_units，关于LSTM和如何使用请看前言的教程。
在使用Tensorflow跑LSTM的试验中， 有个num_units的参数，这个参数是什么意思呢？

先总结一下，num_units这个参数的大小就是LSTM输出结果的维度。例如num_units=128， 那么LSTM网络最后输出就是一个128维的向量。

我们先换个角度举个例子，最后再用公式来说明。

假设在我们的训练数据中，每一个样本 x 是 28*28 维的一个矩阵，那么将这个样本的每一行当成一个输入，通过28个时间步骤展开LSTM，在每一个LSTM单元，我们输入一行维度为28的向量，如下图所示。

![img](http://www.mtcnn.com/wp-content/uploads/2018/09/num_units.png)

那么，对每一个LSTM单元，参数 num_units=128 的话，就是每一个单元的输出为 128*1 的向量，在展开的网络维度来看，如下图所示，对于每一个输入28维的向量，LSTM单元都把它映射到128维的维度， 在下一个LSTM单元时，LSTM会接收上一个128维的输出，和新的28维的输入，处理之后再映射成一个新的128维的向量输出，就这么一直处理下去，知道网络中最后一个LSTM单元，输出一个128维的向量。

![img](http://www.mtcnn.com/wp-content/uploads/2018/09/unfold.png)

从LSTM的公式的角度看是什么原理呢？我们先看一下LSTM的结构和公式：

![img](http://www.mtcnn.com/wp-content/uploads/2018/09/lstm-f.png)

参数 num_units=128 的话，

1. 对于公式 (1) ，h=128*1 维， x=28*1 维，[h,x]便等于156*1 维，W=128*156 维，所以 W*[h,x]=128*156 * 156*1=128*1, b=128*1 维, 所以 f=128*1+128*1=128*1 维；
2. 对于公式 (2) 和 (3)，同上可分析得 i=128*1 维，C(~)=128*1 维;
3. 对于公式 (4) ，f(t)=128*1, C(t-1)=128*1, f(t) .* C(t-1) = 128*1 .* 128*1 = 128*1 , 同理可得 C(t)=128*1 维;
4. 对于公式 (5) 和 (6) ， 同理可得 O=128*1 维， h=O.*tanh(C)=128*1 维。

所以最后LSTM单元输出的h就是 128*1 的向量。

另外几个需要注意的地方：
![img](http://www.mtcnn.com/wp-content/uploads/2018/09/v2-9f5ac19b5fe7df4985836085468563d2_hd.jpg)

1、 cell 的状态是一个向量，是有**多个值**的。。。一开始没有理解这点的时候怎么都想不明白

2、 上一次的状态 h(t-1)是怎么和下一次的输入 x(t) 结合（concat）起来的，这也是很多资料没有明白讲的地方，也很简单，concat， 直白的说就是把**二者直接拼起来**，比如 x是28位的向量，h(t-1)是128位的，那么拼起来就是156位的向量，就是这么简单。。

3、 cell 的**权重是共享**的，这是什么意思呢？这是指这张图片上有三个绿色的大框，代表三个 cell 对吧，但是实际上，它只是代表了**一个 cell 在不同时序时候的状态**，所有的数据只会通过一个 cell，然后不断更新它的权重。

4、那么一层的 LSTM 的参数有多少个？根据第 3 点的说明，我们知道参数的数量是由 cell 的数量决定的，这里只有一个 cell，所以**参数的数量就是这个 cell 里面用到的参数个数**。假设 num_units 是128，输入是28位的，那么根据上面的第 2 点，可以得到，四个小黄框的参数一共有 （128+28）*（128*4），也就是156 * 512，可以看看 TensorFlow 的最简单的 LSTM 的案例，中间层的参数就是这样，不过还要加上输出的时候的激活函数的参数，假设是10个类的话，就是128*10的 W 参数和10个bias 参数

5、cell 最上面的一条线的状态即 s(t) 代表了**长时记忆**，而下面的 h(t)则代表了**工作记忆或短时记忆**